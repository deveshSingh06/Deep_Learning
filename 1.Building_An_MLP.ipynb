{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1.Building_An_MLP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMPC01rdg9J9YulwbmEN0f9"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6DUUh0JvJHr9","colab_type":"text"},"source":["# Steps Involved In Building An MLP Neural Network:"]},{"cell_type":"markdown","metadata":{"id":"-V7tGJ_P21lx","colab_type":"text"},"source":["### **1.Preprocess The Data**:"]},{"cell_type":"markdown","metadata":{"id":"RAkrs5Sj3AGQ","colab_type":"text"},"source":[">- As per the needs of the goal, perform data cleaning such as deduplication, removing unnecessary elements such as urls, etc.(if dealing with textual data)\n","- Convert textual data into numerical vectors.(if dealing with textual data)\n","- Do not forget to normalize the data."]},{"cell_type":"markdown","metadata":{"id":"xbMJhJ0a3ACx","colab_type":"text"},"source":["### **2.Choosing Architecture**:"]},{"cell_type":"markdown","metadata":{"id":"o7izpIXO3AAS","colab_type":"text"},"source":[">   - As per the needs of the goal:\n","      - Select an approriate number of layers of the MLP to build.\n","      - Select an approriate number of neurons in each layer.\n","      - Appropriate number of layers/neurons could be selected by performing hyperparameter tuning over various numbers of layers/neurons."]},{"cell_type":"markdown","metadata":{"id":"Zn-gdCqN2_9t","colab_type":"text"},"source":["### **3.Weight Initialization**:"]},{"cell_type":"markdown","metadata":{"id":"dQV8hQNI2_7a","colab_type":"text"},"source":[">  - Select an appropriate (using hyperparameter tuning) random  weight initialization scheme such as:\n","      - Start with all weights = 0 (rarely used)\n","      - Uniform Initialization (suitable with Sigmoid activation function)\n","      - Xavier/Glorot Initialization:\n","        - Uniform\n","        - Normal\n","      - He Initialization\n","        - Uniform\n","        - Normal\n","      - For more weight initialization schemes, check Keras documentation [here](https://keras.io/api/layers/initializers/)."]},{"cell_type":"markdown","metadata":{"id":"LSkx759j2_5C","colab_type":"text"},"source":["### **4.Choosing Activaion Function**:"]},{"cell_type":"markdown","metadata":{"id":"8FVuIvip2_2V","colab_type":"text"},"source":[">   - Select an appropriate (using hyperparameter tuning) activaion function such as:\n","     - Sigmoid (should be avoided becuase of 'Vanishing Gradients' problem)\n","     - Tanh (should be avoided becuase of 'Vanishing Gradients' problem)\n","     - ReLu (preferred for regression tasks)\n","     - Softmax (preferred for classification tasks)\n","     - For more activation functions, check Keras documentation [here](https://keras.io/api/layers/activations/).\n"]},{"cell_type":"markdown","metadata":{"id":"c_XB7E3-2_y0","colab_type":"text"},"source":["### **5.Choosing optimizer**:"]},{"cell_type":"markdown","metadata":{"id":"yShjBvB52_wA","colab_type":"text"},"source":["> - We generally avoid using SGD for optimization purposes in deep learning (unlike in machine learning) due to the problem of 'Saddle points' in a curve, which are not handled properly by SGD.\n","- Hence, we select other optimization techniques such as:\n","  - Adam\n","  - Adadelta\n","  - Adagrad\n","  - Adamax\n","  - For more optimizers, check Keras documentation [here](https://keras.io/api/optimizers/)."]},{"cell_type":"markdown","metadata":{"id":"MZYwKsew2_tU","colab_type":"text"},"source":["### **6.Using Batch Normalization**(Optional):"]},{"cell_type":"markdown","metadata":{"id":"CaGrAO-m2_px","colab_type":"text"},"source":["> - Generally, batch normalization is used in the cases of very deep layered MLPs.\n","- A small change in the values of weights in the initial layers of the MLP could lead to large changes in the deep layers.\n","- Hence, before the small changes lead to large changes we batch-normalise the deep layers in an MLP."]},{"cell_type":"markdown","metadata":{"id":"V1JHhUHS2_lm","colab_type":"text"},"source":["### **7.Using Dropouts**(Optional):"]},{"cell_type":"markdown","metadata":{"id":"RxoW3ui75GE0","colab_type":"text"},"source":["> - Dropouts refers to dropping out a few number of neurons in a layer, which results in regularising the MLP.\n","- A certain number of neurons turn off at random, hence their outputs are not used in the model building.\n","- Regularization is used for 'Bias-Variance Trade-offs', which refers to 'Overfitting' or 'Underfitting' a model."]},{"cell_type":"markdown","metadata":{"id":"n1D_BJ8V5Flo","colab_type":"text"},"source":["### **8.Choosing Loss Function**:"]},{"cell_type":"markdown","metadata":{"id":"EbZGjcBW5Fi6","colab_type":"text"},"source":[">  - Some of the loss functions used to compute the quantity that a model should seek to minimize during training are:\n","     - Cross-Entropy (preferred for classification tasks)\n","     - Mean Squared Error (preferred for regression tasks)\n","     - Mean Absolute Error\n","     - For more Loss functions, check Keras documentation [here](https://keras.io/api/losses/)."]},{"cell_type":"markdown","metadata":{"id":"kTbd_MzC5FeQ","colab_type":"text"},"source":["### **9.Gradient clipping**(If need be):"]},{"cell_type":"markdown","metadata":{"id":"XH-dv6ep5fYW","colab_type":"text"},"source":["> - Some activation functions cause 'Vanishing Gradients' problem and 'Exploding Gradients' problem.\n","- To overcome these problems, we perform gradient clipping."]},{"cell_type":"markdown","metadata":{"id":"cqmsRQ8_5e07","colab_type":"text"},"source":["### **10.Plot Graphs**:"]},{"cell_type":"markdown","metadata":{"id":"8BBfRxJT5exl","colab_type":"text"},"source":["> - 'Test Loss vs Number of Epochs' should be plotted for model evaluation."]},{"cell_type":"code","metadata":{"id":"ToDZlxqRId1M","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}