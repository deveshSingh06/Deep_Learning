{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1.Building_An_MLP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOwyUOrpLZ9hOivKgAwZMd2"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6DUUh0JvJHr9","colab_type":"text"},"source":["# Steps Involved In Building An MLP:"]},{"cell_type":"markdown","metadata":{"id":"5bmfXbExJUNZ","colab_type":"text"},"source":["\n","\n","1.   **Preprocess The Data**:\n","> - As per the needs of the goal, perform data cleaning such as deduplication, removing unnecessary elements such as urls, etc.(if dealing with textual data)\n","  - Convert textual data into numerical vectors.(if dealing with textual data)\n","  - Do not forget to normalize the data.\n","\n","\n","2.   **Choosing Architecture**:\n",">   - As per the needs of the goal:\n","      - Select an approriate number of layers of the MLP to build.\n","      - Select an approriate number of neurons in each layer.\n","      - Appropriate number of layers/neurons could be selected by performing hyperparameter tuning over various numbers of layers/neurons.\n","\n","\n","3.  **Weight Initialization**:\n"," >  - Select an appropriate (using hyperparameter tuning) random  weight initialization scheme such as:\n","      - Start with all weights = 0 (rarely used)\n","      - Uniform Initialization (suitable with Sigmoid activation function)\n","      - Xavier/Glorot Initialization:\n","        - Uniform\n","        - Normal\n","      - He Initialization\n","        - Uniform\n","        - Normal\n","      - For more weight initialization schemes, check Keras documentation [here](https://keras.io/api/layers/initializers/).\n","\n","\n","4.  **Choosing Activaion Function**:\n",">   - Select an appropriate (using hyperparameter tuning) activaion function such as:\n","     - Sigmoid (should be avoided becuase of 'Vanishing Gradients' problem)\n","     - Tanh (should be avoided becuase of 'Vanishing Gradients' problem)\n","     - ReLu (preferred for regression tasks)\n","     - Softmax (preferred for classification tasks)\n","     - For more activation functions, check Keras documentation [here](https://keras.io/api/layers/activations/).\n","\n","\n","5. **Choosing optimizer**:\n"," >  - We generally avoid using SGD for optimization purposes in deep learning (unlike in machine learning) due to the problem of 'Saddle points' in a curve, which are not handled properly by SGD.\n","    - Hence, we select other optimization techniques such as:\n","      - Adam\n","      - Adadelta\n","      - Adagrad\n","      - Adamax\n","      - For more optimizers, check Keras documentation [here](https://keras.io/api/optimizers/).\n","\n","\n","6. **Using Batch Normalization**(Optional):\n"," >   - Generally, batch normalization is used in the cases of very deep layered MLPs.\n","     - A small change in the values of weights in the initial layers of the MLP could lead to large changes in the deep layers.\n","     - Hence, before the small changes lead to large changes we batch-normalise the deep layers in an MLP. \n","\n","\n","7. **Using Dropouts**(Optional):\n",">   - Dropouts refers to dropping out a few number of neurons in a layer, which results in regularising the MLP.\n","     - A certain number of neurons turn off at random, hence their outputs are not used in the model building.\n","     - Regularization is used for 'Bias-Variance Trade-offs', which refers to 'Overfitting' or 'Underfitting' a model.\n","\n","\n","8. **Choosing Loss Function**:\n",">  - Some of the loss functions used to compute the quantity that a model should seek to minimize during training are:\n","     - Cross-Entropy (preferred for classification tasks)\n","     - Mean Squared Error (preferred for regression tasks)\n","     - Mean Absolute Error\n","     - For more Loss functions, check Keras documentation [here](https://keras.io/api/losses/).\n","\n","\n","9. **Gradient clipping**(If need be):\n"," >  - Some activation functions cause 'Vanishing Gradients' problem and 'Exploding Gradients' problem.\n","    - To overcome these problems, we perform gradient clipping.\n","\n","\n","10. **Plot Graphs**:\n","   - 'Test Loss vs Number of Epochs' should be plotted for model evaluation.\n"]},{"cell_type":"code","metadata":{"id":"ToDZlxqRId1M","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}